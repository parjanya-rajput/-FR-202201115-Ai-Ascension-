{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qCEEgjKHLYPqiEZz2k_DnflsYpczOVW1",
      "authorship_tag": "ABX9TyPsaQv6/88NqrA/nc4s91gn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parjanya-rajput/-FR-202201115-Ai-Ascension-/blob/main/FaceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WELCOM TO THE FACE RECOGNITION SYSTEM BUILT USING SIAMESE NETWORK"
      ],
      "metadata": {
        "id": "Z7pheR9CDO7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAW6EUe7Eh4M",
        "outputId": "1ae7c177-3e89-4f11-ecf5-507ecd00705c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import uuid\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "#Tensorflow dependencies\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "R_QWm6q4FtF-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting Paths\n",
        "POS_PATH = os.path.join('data', 'positive')\n",
        "NEG_PATH = os.path.join('data', 'negative')\n",
        "ANC_PATH = os.path.join('data', 'anchor')"
      ],
      "metadata": {
        "id": "dOOjUoexaxLc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make Folders\n",
        "os.makedirs(POS_PATH)\n",
        "os.makedirs(NEG_PATH)\n",
        "os.makedirs(ANC_PATH)"
      ],
      "metadata": {
        "id": "THqhUTQ_bXyn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Labbelled faces in **WILD DATASET**\n",
        "URL: http://vis-www.cs.umass.edu/lfw/"
      ],
      "metadata": {
        "id": "Y2g4oMdvea7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncompressing the Tar GZ file lfw\n",
        "!tar -xf lfw.tgz"
      ],
      "metadata": {
        "id": "I040jq4_fDUy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Moving all the wild face data to negative for training the model\n",
        "for directory in os.listdir('lfw'):\n",
        "  for f in os.listdir(os.path.join('lfw', directory)):\n",
        "    EX_PATH = os.path.join('lfw', directory, f)\n",
        "    NEW_PATH = os.path.join(NEG_PATH, f)\n",
        "    os.replace(EX_PATH, NEW_PATH)"
      ],
      "metadata": {
        "id": "SDPhcEpaj616"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for removing all the lfw set files and saving space\n",
        "for dir in os.listdir('lfw'):\n",
        "  path = os.path.join('lfw', dir)\n",
        "  os.rmdir(path)"
      ],
      "metadata": {
        "id": "h4czh75ejWH_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Functions for capturing Image in Collab"
      ],
      "metadata": {
        "id": "MfBHjVaujwre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "mcOtn3TRjmwT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haar Cascade Classifier"
      ],
      "metadata": {
        "id": "KvmVE_Frj7NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ],
      "metadata": {
        "id": "zYNtC00TyCg1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='{}.jpg'.format(uuid.uuid1()), quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve)\n",
        "        \n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "      \n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data) \n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      # img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "      img = img[y:y+h, x:x+w]\n",
        "      print(x,y,w,h)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ],
      "metadata": {
        "id": "FOx0OWCzyG9d"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  #Just change ANC_PATH to  POS_PATH for stroing positive image\n",
        "  filename = take_photo(os.path.join(ANC_PATH,'{}.jpg'.format(uuid.uuid1())))\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "wJxaH-mOyG1Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "39914ca8-e248-429f-ec8f-7748a823e180"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve)\n",
              "        \n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "      \n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(480, 640)\n",
            "253 194 177 177\n",
            "Saved to data/anchor/0c11a322-beb0-11ed-809b-0242ac1c000c.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change jpg to png for differnet type of image \n",
        "anchor = tf.data.Dataset.list_files(ANC_PATH+'/*.jpg').take(300)\n",
        "positive = tf.data.Dataset.list_files(POS_PATH+'/*.jpg').take(300)\n",
        "negative = tf.data.Dataset.list_files(NEG_PATH+'/*.jpg').take(300)"
      ],
      "metadata": {
        "id": "hhsjKbnxYhry"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_test = anchor.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "mO-hCknsZS7D"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_test.next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdJdkHodZZaJ",
        "outputId": "ee35a3bd-f452-4aed-adb5-e3b335b1b10a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'data/anchor/d03ac726-bde6-11ed-ad76-46baee1e3e63.jpg'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Function"
      ],
      "metadata": {
        "id": "lO-MqqCRaxDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file_path):\n",
        "\n",
        "  #Reading the image from the filepath\n",
        "  byte_img = tf.io.read_file(file_path)\n",
        "\n",
        "  #Loading the image from filepath\n",
        "  img = tf.io.decode_jpeg(byte_img)\n",
        "\n",
        "  #Resizing the image\n",
        "  img = tf.image.resize(img, (105,105))\n",
        "\n",
        "  #Scaling the image. to fit in 0 to 1\n",
        "  img = img / 255.0\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "3K1MrnQqawu3"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = preprocess('data/anchor/e6bfa924-bdd9-11ed-aec5-0242ac1c000c.jpg')"
      ],
      "metadata": {
        "id": "5OVULcdVbXh5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img.numpy().max()"
      ],
      "metadata": {
        "id": "rMQlDjs8bqbj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(img)"
      ],
      "metadata": {
        "id": "jn5iJGAVbe9q"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I am trying to create a labelled dataset"
      ],
      "metadata": {
        "id": "7_s8JiP3dEje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What it will be resulting\n",
        "# (anchor,positive) => 1,1,1,1,1\n",
        "# (anchor,negative) => 0,0,0,0,0"
      ],
      "metadata": {
        "id": "zlL7oSChdJfE"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "data = positives.concatenate(negatives)"
      ],
      "metadata": {
        "id": "4szEfjskdWgp"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = data.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "v4sH9-fVeDmz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = samples.next()"
      ],
      "metadata": {
        "id": "2HmPAA_ye_-V"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIsRIm6lgLPg",
        "outputId": "8c609299-7f80-4268-99e1-68e4eb5f9e45"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(b'data/anchor/d6722670-bde6-11ed-ad76-46baee1e3e63.jpg',\n",
              " b'data/positive/b665e3a8-bde6-11ed-ad76-46baee1e3e63.jpg',\n",
              " 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing the Partition"
      ],
      "metadata": {
        "id": "Sn7ePy4pfcyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_twin(input_img, validation_img, label):\n",
        "  return(preprocess(input_img), preprocess(validation_img), label)"
      ],
      "metadata": {
        "id": "aJ4SjyWAfgEA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = preprocess_twin(*exp)"
      ],
      "metadata": {
        "id": "tVDMjDGqf-IL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res[2]"
      ],
      "metadata": {
        "id": "smKFS6vUhH83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda9628c-0f5a-4dd4-d3c6-dd68a0058e58"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building DATALOADER PIPELINE"
      ],
      "metadata": {
        "id": "Nt_IA1cWhqu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(preprocess_twin)\n",
        "data = data.cache()\n",
        "data.shuffle(buffer_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RRv-Ot2ht-8",
        "outputId": "cf66c72b-a20d-4a62-f999-f497405c43df"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ShuffleDataset element_spec=(TensorSpec(shape=(105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = data.as_numpy_iterator()"
      ],
      "metadata": {
        "id": "o48DkbAsihpU"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samp = samples.next()"
      ],
      "metadata": {
        "id": "BIEExSe6ioXw"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Partition\n",
        "train_data = data.take(round(len(data)*.7))\n",
        "train_data = train_data.batch(16)\n",
        "train_data = train_data.prefetch(8)"
      ],
      "metadata": {
        "id": "YHCjlYyvitL0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hNZwyUHkHR5",
        "outputId": "f014afea-cedc-4002-8ba6-b42d6bdb0e4c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 105, 105, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Data\n",
        "test_data = data.skip(round(len(data)*.7))\n",
        "test_data = test_data.take(round(len(data)*.3))\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ],
      "metadata": {
        "id": "amyBrze0kYNv"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EMBEDDING LAYER**"
      ],
      "metadata": {
        "id": "sXWkPsW4Xijb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(105,105,3), name = 'input_img')"
      ],
      "metadata": {
        "id": "WvtlA39QYyT-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = Conv2D(64, (10,10), activation='relu')(inp)"
      ],
      "metadata": {
        "id": "4rvN7pHpY4EQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nui-p1CJZTGv",
        "outputId": "66b6690e-3ddd-4e1e-c493-011226490438"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 96, 96, 64) dtype=float32 (created by layer 'conv2d_8')>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)"
      ],
      "metadata": {
        "id": "cAHl0ORhZ4CE"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo8cWD4uZ3sM",
        "outputId": "b9a3e6b7-35c1-4c48-d3a0-acec78bd4a92"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 48, 48, 64) dtype=float32 (created by layer 'max_pooling2d_6')>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)"
      ],
      "metadata": {
        "id": "Tsh25NETathS"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDu3ZmKpatbT",
        "outputId": "41743a25-2648-4c55-e492-f4a51ef9f910"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 42, 42, 128) dtype=float32 (created by layer 'conv2d_9')>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUnwvCSNatTf",
        "outputId": "d1e73181-39de-4045-b877-c16819a045dd"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 21, 21, 128) dtype=float32 (created by layer 'max_pooling2d_7')>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)"
      ],
      "metadata": {
        "id": "MveqyQkKdAKd"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLv8p0SidAIX",
        "outputId": "9dc4fdf9-315b-4f15-cdd5-2cbceced97e1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 9, 9, 128) dtype=float32 (created by layer 'max_pooling2d_8')>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
        "f1 = Flatten()(c4)\n",
        "d1 = Dense(4096, activation='sigmoid')(f1)"
      ],
      "metadata": {
        "id": "hT9HHCn4dAGB"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flatten results c4=> 6*6*256 = 9216"
      ],
      "metadata": {
        "id": "yUG6iAnEdfyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiIKywQodADa",
        "outputId": "998a5bb2-dfd1-45f8-8023-a19fe272de5e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 9216) dtype=float32 (created by layer 'flatten_2')>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gYNrlA0dAAl",
        "outputId": "cdf2e0ff-31a7-4dc5-95be-ca1e2612ecf6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'dense_4')>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod = Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "pDFHIU9gofn8"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za4shxElopuR",
        "outputId": "238bd843-c37e-44ad-e958-2ad80d33aff7"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_img (InputLayer)      [(None, 105, 105, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 96, 96, 64)        19264     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 48, 48, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 42, 42, 128)       401536    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 21, 21, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 18, 18, 128)       262272    \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 9, 9, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 6, 6, 256)         524544    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              37752832  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embedding():\n",
        "  #Input image 105x105\n",
        "  inp = Input(shape=(105,105,3), name='input_image')\n",
        "\n",
        "  #Applying Convulution layer\n",
        "  c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
        "\n",
        "  # Maxpooling the c1 layer\n",
        "  m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
        "\n",
        "  #Second Block for neural network\n",
        "  c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "  m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
        "\n",
        "  #Third Block for neural network\n",
        "  c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "  m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
        "\n",
        "  #Final convolution then flatten and densing the layer i.e embedding layer\n",
        "  c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
        "  f1 = Flatten()(c4)\n",
        "  d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "  \n",
        "\n",
        "\n",
        "  return Model(inputs=[inp], outputs=[d1], name='embedding')"
      ],
      "metadata": {
        "id": "RBZsxxFyXhtR"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = make_embedding()"
      ],
      "metadata": {
        "id": "1baLOlhckylv"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0eI_z1opJTV",
        "outputId": "2d575873-8bc1-4af6-db1c-7bb31f843e92"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"embedding\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_image (InputLayer)    [(None, 105, 105, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 96, 96, 64)        19264     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 48, 48, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 42, 42, 128)       401536    \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 21, 21, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 18, 18, 128)       262272    \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 9, 9, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 6, 6, 256)         524544    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4096)              37752832  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 38,960,448\n",
            "Trainable params: 38,960,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building DISTANCE LAYER\n",
        "This is the defining characteristic in Siamese Neural Network used for Keras layering"
      ],
      "metadata": {
        "id": "SbQVD4ySpdJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class L1Dist(Layer):\n",
        "\n",
        "  #Inheritance\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    #Turning the feature vector(d1) into one single output\n",
        "    def call(self, input_embedding, validation_embedding):\n",
        "      return tf.math.abs(input_embedding - validation_embedding)"
      ],
      "metadata": {
        "id": "FPrUURKypLkI"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_layer = L1Dist()"
      ],
      "metadata": {
        "id": "mDEtV1RzrOR_"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIAMESE NEURAL NETWORK"
      ],
      "metadata": {
        "id": "Icao78XSryEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = Input(name='input_img', shape=(105,105,3))\n",
        "validation_image = Input(name='validation_img', shape=(105,105,3))"
      ],
      "metadata": {
        "id": "zWI-fyzptvG2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp_embedding = embedding(input_image)\n",
        "val_embedding = embedding(validation_image)"
      ],
      "metadata": {
        "id": "D6p-ntjDtvEQ"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = siamese_layer(inp_embedding, val_embedding)"
      ],
      "metadata": {
        "id": "eq0hIOs4tu9Q"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = Dense(1, activation='sigmoid')(distances)"
      ],
      "metadata": {
        "id": "mTCJrTG9u3a3"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag8AlR-Cu66B",
        "outputId": "43aecb91-ff72-4faf-f1b6-8b8b166ead91"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_6')>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_network = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
      ],
      "metadata": {
        "id": "03LRAZ5qvOjS"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-eU4JLqvRi-",
        "outputId": "52c3c26c-8162-47dd-d887-c84d4c43b471"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SiameseNetwork\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
            "                                                                  'validation_img[0][0]']         \n",
            "                                                                                                  \n",
            " l1_dist_2 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
            "                                                                  'embedding[1][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 1)            4097        ['l1_dist_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_siamese_model():\n",
        "\n",
        "  #Handling the input images\n",
        "  input_image = Input(name='input_img', shape=(105,105,3))\n",
        "  validation_image = Input(name='validation_img', shape=(105,105,3))\n",
        "\n",
        "  #Siamese Distance components\n",
        "  siamese_layer = L1Dist()\n",
        "  siamese_layer._name = 'Distance'\n",
        "  distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
        "\n",
        "  #Classification \n",
        "  classifier = Dense(1, activation='sigmoid')(distances)\n",
        "\n",
        "  return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n"
      ],
      "metadata": {
        "id": "O7DZ1-2pr6r6"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = make_siamese_model()"
      ],
      "metadata": {
        "id": "dmq0kNimvh0n"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j9e5JsLvlIm",
        "outputId": "2b076ab0-9b6f-410e-b05a-271fe5bb4b03"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"SiameseNetwork\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_img (InputLayer)         [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " validation_img (InputLayer)    [(None, 105, 105, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " embedding (Functional)         (None, 4096)         38960448    ['input_img[0][0]',              \n",
            "                                                                  'validation_img[0][0]']         \n",
            "                                                                                                  \n",
            " Distance (L1Dist)              (None, 4096)         0           ['embedding[2][0]',              \n",
            "                                                                  'embedding[3][0]']              \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1)            4097        ['Distance[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,964,545\n",
            "Trainable params: 38,964,545\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss and Optimiser Setup**"
      ],
      "metadata": {
        "id": "PBs0JKbqqSFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_cross_loss = tf.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "K8_xnTBaqRfK"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "ZksxkddGqRdF"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint Components**"
      ],
      "metadata": {
        "id": "O5oHEphir6Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cp = os.path.join('training_checkpoints')\n",
        "os.makedirs(cp)\n",
        "checkpoint_dir = './training_checkpoint'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
      ],
      "metadata": {
        "id": "g5rDO6uCqRay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Step Function**"
      ],
      "metadata": {
        "id": "F-qM2GfTtroK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(batch):\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    #Fetch Anchor Images\n",
        "    X = batch[:2]\n",
        "    #Fetch Label\n",
        "    y = batch[2]\n",
        "\n",
        "    #predict \n",
        "    yhat = siamese_model(X, training=True)\n",
        "\n",
        "    #Calc Loss\n",
        "    loss = binary_cross_loss(y, yhat)\n",
        "  print(loss)\n",
        "\n",
        "  #calculate gradients\n",
        "  grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
        "\n",
        "  #Calculate updated weights and apply to siamese model\n",
        "  opt.apply_gradients(zip(grad, siamese_model.trainable_variables))  \n",
        "  return loss"
      ],
      "metadata": {
        "id": "X-noqppYtwG5"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "xiKH4Ih6yENO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data, EPOCHS):\n",
        "  #Loop through all epochs\n",
        "  for epoch in range(1, EPOCHS+1):\n",
        "    print(('\\n EPOCH {}/{}'.format(epoch, EPOCHS)))\n",
        "    progbar = tf.keras.utils.Progbar(len(data))\n",
        "\n",
        "    #Looping through each batch\n",
        "    for idx, batch in enumerate(data):\n",
        "      #Running Train Step \n",
        "      train_step(batch)\n",
        "      progbar.update(idx+1)\n",
        "\n",
        "      #Save the checkpoints\n",
        "      if epoch % 10 == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_prefix)\n"
      ],
      "metadata": {
        "id": "wq5yQbXfyGOH"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**"
      ],
      "metadata": {
        "id": "jaOb3SsP0Die"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "g-l9Lka5zx-x"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_data, EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUa7oNk11sTi",
        "outputId": "1474b6e9-cfe9-42c8-e2e8-35f52edf44e4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " EPOCH 1/50\n",
            "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "13/14 [==========================>...] - ETA: 0sTensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
            "14/14 [==============================] - 15s 137ms/step\n",
            "\n",
            " EPOCH 2/50\n",
            "14/14 [==============================] - 2s 118ms/step\n",
            "\n",
            " EPOCH 3/50\n",
            "14/14 [==============================] - 1s 110ms/step\n",
            "\n",
            " EPOCH 4/50\n",
            "14/14 [==============================] - 1s 111ms/step\n",
            "\n",
            " EPOCH 5/50\n",
            "14/14 [==============================] - 1s 110ms/step\n",
            "\n",
            " EPOCH 6/50\n",
            "14/14 [==============================] - 1s 111ms/step\n",
            "\n",
            " EPOCH 7/50\n",
            "14/14 [==============================] - 1s 111ms/step\n",
            "\n",
            " EPOCH 8/50\n",
            "14/14 [==============================] - 1s 111ms/step\n",
            "\n",
            " EPOCH 9/50\n",
            "14/14 [==============================] - 1s 110ms/step\n",
            "\n",
            " EPOCH 10/50\n",
            "14/14 [==============================] - 8s 628ms/step\n",
            "\n",
            " EPOCH 11/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 12/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 13/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 14/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 15/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 16/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 17/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 18/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 19/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 20/50\n",
            "14/14 [==============================] - 8s 619ms/step\n",
            "\n",
            " EPOCH 21/50\n",
            "14/14 [==============================] - 2s 113ms/step\n",
            "\n",
            " EPOCH 22/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 23/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 24/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 25/50\n",
            "14/14 [==============================] - 1s 106ms/step\n",
            "\n",
            " EPOCH 26/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 27/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 28/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 29/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 30/50\n",
            "14/14 [==============================] - 8s 619ms/step\n",
            "\n",
            " EPOCH 31/50\n",
            "14/14 [==============================] - 1s 107ms/step\n",
            "\n",
            " EPOCH 32/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 33/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 34/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 35/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 36/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 37/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 38/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 39/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 40/50\n",
            "14/14 [==============================] - 8s 632ms/step\n",
            "\n",
            " EPOCH 41/50\n",
            "14/14 [==============================] - 2s 113ms/step\n",
            "\n",
            " EPOCH 42/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 43/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 44/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 45/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 46/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 47/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 48/50\n",
            "14/14 [==============================] - 1s 108ms/step\n",
            "\n",
            " EPOCH 49/50\n",
            "14/14 [==============================] - 1s 109ms/step\n",
            "\n",
            " EPOCH 50/50\n",
            "14/14 [==============================] - 8s 616ms/step\n"
          ]
        }
      ]
    }
  ]
}